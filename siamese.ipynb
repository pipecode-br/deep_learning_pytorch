{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pipecode-br/deep_learning_pytorch/blob/main/siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Etapa 1: Importar biblioteca"
      ],
      "metadata": {
        "id": "TEqVuMubgeYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "torch.__version__"
      ],
      "metadata": {
        "id": "rkO1JsrweBzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "!pip install pylabel"
      ],
      "metadata": {
        "id": "25aKmeF_heUj",
        "outputId": "bc07a0e3-94e9-4e97-ab5b-a4f53e07dc36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-0.2.4.tar.gz (15 kB)\n",
            "Collecting certifi==2021.5.30\n",
            "  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 31.1 MB/s \n",
            "\u001b[?25hCollecting chardet==4.0.0\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 57.3 MB/s \n",
            "\u001b[?25hCollecting cycler==0.10.0\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.10)\n",
            "Collecting kiwisolver==1.3.1\n",
            "  Downloading kiwisolver-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 44.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from roboflow) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.21.6)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (7.1.2)\n",
            "Collecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-0.20.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.23.0)\n",
            "Collecting requests_toolbelt\n",
            "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.15.0)\n",
            "Collecting urllib3==1.26.6\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 59.9 MB/s \n",
            "\u001b[?25hCollecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.64.0)\n",
            "Collecting PyYAML>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.0 MB/s \n",
            "\u001b[?25hCollecting requests\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->roboflow) (2.0.12)\n",
            "Building wheels for collected packages: roboflow, wget\n",
            "  Building wheel for roboflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for roboflow: filename=roboflow-0.2.4-py3-none-any.whl size=21356 sha256=a8c935b51e3744cb9b71c1ebc4ff2fe6a452bf74a9f0b724edd1fbfdbb39d578\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/2c/60/3733c0e323b1d7aec00f852264314bf6c48192c8e1dc92b3ee\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=5fa4669a6d6355c5375ba6fd42c43c1464aaf9af801b38b90aaacfb19033558d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built roboflow wget\n",
            "Installing collected packages: urllib3, certifi, requests, pyparsing, kiwisolver, cycler, wget, requests-toolbelt, PyYAML, python-dotenv, chardet, roboflow\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2021.10.8\n",
            "    Uninstalling certifi-2021.10.8:\n",
            "      Successfully uninstalled certifi-2021.10.8\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.8\n",
            "    Uninstalling pyparsing-3.0.8:\n",
            "      Successfully uninstalled pyparsing-3.0.8\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.2\n",
            "    Uninstalling kiwisolver-1.4.2:\n",
            "      Successfully uninstalled kiwisolver-1.4.2\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.11.0\n",
            "    Uninstalling cycler-0.11.0:\n",
            "      Successfully uninstalled cycler-0.11.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 3.0.4\n",
            "    Uninstalling chardet-3.0.4:\n",
            "      Successfully uninstalled chardet-3.0.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-6.0 certifi-2021.5.30 chardet-4.0.0 cycler-0.10.0 kiwisolver-1.3.1 pyparsing-2.4.7 python-dotenv-0.20.0 requests-2.27.1 requests-toolbelt-0.9.1 roboflow-0.2.4 urllib3-1.26.6 wget-3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "chardet",
                  "cycler",
                  "kiwisolver",
                  "pyparsing",
                  "requests",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "id": "1VE98TgFhLAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Etapa 2: Construir modelo"
      ],
      "metadata": {
        "id": "cxP8toDUhACJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class feature(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "      super().__init__()\n",
        "      # load resnet-18\n",
        "      resnet18 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "      #primeiras camadas conv do resnet\n",
        "      self.layer1 = nn.Sequential(\n",
        "          resnet18.conv1,\n",
        "          resnet18.bn1,\n",
        "          resnet18.relu,\n",
        "          resnet18.maxpool\n",
        "        )\n",
        "      self.layer2 = nn.Sequential(\n",
        "          resnet18.conv1,\n",
        "          resnet18.bn1,\n",
        "          resnet18.relu,\n",
        "          resnet18.maxpool\n",
        "        )\n",
        "      self.layer1.training = False\n",
        "  def forward(self,X):\n",
        "    X_ = self.layer1(X)\n",
        "    return X_\n",
        "\n",
        "class discriminator(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "      super().__init__()\n",
        "      self.flatten = nn.Flatten()\n",
        "      size = 115200\n",
        "      self.dense1 = nn.Linear(in_features=size,out_features=128)\n",
        "      self.dense2 = nn.Linear(in_features=128,out_features=36)\n",
        "      self.dense3 = nn.Linear(in_features=36,out_features=1)\n",
        "      self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "  def forward(self,X1,X2):\n",
        "    X1_ = self.flatten(X1)\n",
        "    X2_ = self.flatten(X2)\n",
        "    # X1 = X1.view(X1.size()[0], -1)\n",
        "    # X2 = X2.view(X2.size()[0], -1)\n",
        "    x = torch.cat((X1_,X2_), 1)\n",
        "    x = self.dropout(F.leaky_relu(self.dense1(x),0.2))    \n",
        "    x = self.dropout(F.leaky_relu(self.dense2(x),0.2))\n",
        "    x = self.dropout(F.leaky_relu(self.dense3(x),0.2))\n",
        "    return x\n",
        "\n",
        "class siamese(nn.Module):\n",
        "      def __init__(self) -> None:\n",
        "          super().__init__()\n",
        "          self.feature = feature()      \n",
        "          self.discriminator = discriminator()\n",
        "\n",
        "      def forward(self, X1, X2):          \n",
        "          X1_ = self.feature(X1)\n",
        "          X2_ = self.feature(X2)\n",
        "          X = self.discriminator(X1_, X2_)\n",
        "          return X"
      ],
      "metadata": {
        "id": "lZer_dWYevhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class DeepNeuralInterface(ABC,nn.Module):\n",
        "    @abstractmethod\n",
        "    def show(self):\n",
        "      pass\n",
        "    @abstractmethod\n",
        "    def get_transforms(self):\n",
        "      pass\n",
        "      \n",
        "class TrainSiameseInterface(ABC):\n",
        "    @abstractmethod\n",
        "    def train(self,net:nn.Module,X)->None:\n",
        "      pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def train_epoch(self,net:nn.Module,X)->None:\n",
        "      pass\n",
        "\n",
        "class teste(TrainSiameseInterface):\n",
        "  def __init__(self,device) -> None:\n",
        "      super().__init__()\n",
        "      self.device = device\n",
        "\n",
        "  def train(self,net,train_loader,epochs=15)->None:\n",
        "    optimizer = optim.Adam(net.parameters(),lr=0.002)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    net.to(device)\n",
        "    for epoch in range(epochs):\n",
        "      D_running_loss = 0\n",
        "      G_running_loss = 0\n",
        "\n",
        "      for i,(imagens_reais,_) in enumerate(train_loader):\n",
        "\n",
        "        batch_size= imagens_reais.size(0)\n",
        "                   \n",
        "        imagens_reais = imagens_reais * 2-1       \n",
        "        imagens_reais = imagens_reais.to(self.device)\n",
        "\n",
        "        #treinamento com reais\n",
        "        outputs_reais = net.forward(imagens_reais,imagens_reais)\n",
        "        labels_reais = (torch.ones(batch_size)*0.9).to(device)\n",
        "        D_loss_reais = criterion(outputs_reais.view(*labels_reais.shape),labels_reais)      \n",
        "\n",
        "        #treinamento com falso  \n",
        "        imagens_ruido = torch.flip(imagens_reais, [0,1])\n",
        "        outputs_falsos = net.forward(imagens_reais,imagens_ruido)\n",
        "        labels_falsos= torch.zeros(batch_size).to(device)\n",
        "        D_loss_falsos = criterion(outputs_reais.view(*labels_falsos.shape),labels_falsos)\n",
        "\n",
        "        D_loss = D_loss_reais+ D_loss_falsos\n",
        "        D_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        D_running_loss += D_loss.item()\n",
        "        # print('train ',i,imagens_reais.shape,D_loss_reais,D_loss_falsos)\n",
        "\n",
        "      D_running_loss /=len(train_loader)\n",
        "      print('epoch -> {} loss -> {}'.format(epoch,D_running_loss)) \n",
        "\n",
        "\n",
        "  def train_epoch(self,net,X)->None:\n",
        "    pass\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hirfPaX-Nyhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#criar transformador\n",
        "#transform = transforms.ToTensor()\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(128),\n",
        "    transforms.CenterCrop(120),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n"
      ],
      "metadata": {
        "id": "P-GtI6qPT64g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "from skimage.draw import rectangle\n",
        "from roboflow import Roboflow\n",
        "from pylabel import importer\n",
        "from PIL import Image\n",
        "class FibersDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Fibers dataset.\"\"\"\n",
        "\n",
        "    def __init__(self,transform=None):\n",
        "      rf = Roboflow(api_key=\"JhqX7HlUL57cmzbBqIav\")\n",
        "      project = rf.workspace().project(\"fibberpaper\")\n",
        "      dataset = project.version(1).download(\"yolov5\")\n",
        "      dataset = importer.ImportYoloV5(path='/content/FibberPaper-1/test/labels', path_to_images='/content/FibberPaper-1/test/images')\n",
        "      self.mapa,self.data = self.extractFibersOfImagens(dataset.df)\n",
        "      self.sourceTransform= transform\n",
        "      self.rootDir = '.'\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "            \n",
        "        image = Image.fromarray(self.data[idx])\n",
        "        if self.sourceTransform:\n",
        "            image = self.sourceTransform(image)\n",
        "\n",
        "        class_id = torch.tensor([0])\n",
        "        return image,class_id\n",
        "\n",
        "\n",
        "    #extrar o conjunto de pixel das fibras contidas no boundbox descrito no dataframe\n",
        "    def extractFibersOfImagens(self,df):\n",
        "      mapa = {}\n",
        "      fibers = []\n",
        "      #para cada linha \n",
        "      for index, row in df.iterrows():\n",
        "        if \"idoc\" in row.img_filename: \n",
        "          continue\n",
        "        #ler a imagem e salva no map\n",
        "        path_full='{}/{}'.format(row.img_folder,row.img_filename)\n",
        "        if mapa.get(row.img_filename) is None:\n",
        "          img = io.imread(path_full)\n",
        "          mapa[row.img_filename]=img\n",
        "        #recorta o box da fibra\n",
        "        x1,y1,x2,y2 = int(row.ann_bbox_xmin),int(row.ann_bbox_ymin),int(row.ann_bbox_xmax),int(row.ann_bbox_ymax)\n",
        "        fibers.append(mapa[row.img_filename][y1:y2,x1:x2])\n",
        "        #normaliza a imagem com o transforms\n",
        "        #tenta remover o fundo\n",
        "      return mapa,np.array(fibers)\n",
        "\n",
        "\n",
        "train = FibersDataset(transform=preprocess)\n",
        "# criar loader\n",
        "train_loader = torch.utils.data.DataLoader(train,batch_size=16)"
      ],
      "metadata": {
        "id": "cUdN5piCHrdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criação dos objetos"
      ],
      "metadata": {
        "id": "ughFLqD05w-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = siamese()\n",
        "param = net.parameters()\n",
        "optimizer = optim.Adam(param,lr=0.002)\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "qWcyafR-kGUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')"
      ],
      "metadata": {
        "id": "lczWDOsS8q2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teste(device=device).train(net=net,train_loader=train_loader,epochs=500)"
      ],
      "metadata": {
        "id": "VK7oKfkkhWW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img1 =(3 , 120, 120)\n",
        "summary(net,[img1,img1])"
      ],
      "metadata": {
        "id": "WzFRx6e18uzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Separa primeiras camadas"
      ],
      "metadata": {
        "id": "7RF3dd3N_fNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### visualiza resultado"
      ],
      "metadata": {
        "id": "9GIFiWQn_3MF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_result(input_image,conv_unif):\n",
        "  plt.style.use('classic')\n",
        "  plt.imshow(input_image.transpose(2, 0))\n",
        "  plt.show()\n",
        "  plt.imshow(conv_unif)\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "ZA6Gwk8E8dHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testa sub modelo em um exemplo real"
      ],
      "metadata": {
        "id": "M6Lmi6za_pKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_image,_ in train:\n",
        "  # input_image,_ = next(iter(train))\n",
        "  input_batch = input_image.unsqueeze(0) \n",
        "  print(input_batch.shape)\n",
        "\n",
        "  model1 = feature()\n",
        "  # move the input and model to GPU for speed if available\n",
        "  if torch.cuda.is_available():\n",
        "      input_batch = input_batch.to('cuda')\n",
        "      model1.to('cuda')\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output = model1(input_batch)\n",
        "      out1=output.cpu().numpy()\n",
        "  conv_unif = np.sum(out1[0],axis=0)/out1[0].shape[0]\n",
        "  print(conv_unif.shape)\n",
        "  plot_result(input_image,conv_unif)"
      ],
      "metadata": {
        "id": "RG3xxNE7-mC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# testar rede siamesa\n"
      ],
      "metadata": {
        "id": "kGbk_yymbysK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### concatena features da camada de saida"
      ],
      "metadata": {
        "id": "B2V-NImB_w6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch = next(iter(train_loader))\n",
        "for img,_ in train_loader:\n",
        "  print(t)"
      ],
      "metadata": {
        "id": "pa6CeoywfxlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for input_image,_ in train:\n",
        "  # input_image,_ = next(iter(train))\n",
        "  input_batch = input_image.unsqueeze(0) \n",
        "  print(input_batch.shape)\n",
        "  # move the input and model to GPU for speed if available\n",
        "  if torch.cuda.is_available():\n",
        "      input_batch = input_batch.to('cuda')\n",
        "      net = net.to('cuda')\n",
        "  with torch.no_grad():\n",
        "      output = net(input_batch,input_batch)\n",
        "      out1=output.cpu().numpy()\n",
        "      print(out1)"
      ],
      "metadata": {
        "id": "_Ew-Z4rK9F6s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "siamese",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}