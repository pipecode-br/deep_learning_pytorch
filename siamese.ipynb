{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pipecode-br/deep_learning_pytorch/blob/main/siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Etapa 1: Importar biblioteca"
      ],
      "metadata": {
        "id": "TEqVuMubgeYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "torch.__version__"
      ],
      "metadata": {
        "id": "rkO1JsrweBzI",
        "outputId": "e21eb391-a88a-462f-9253-4817dbc479d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.10.0+cu111'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VE98TgFhLAh",
        "outputId": "6e749e05-6620-4867-d80a-a51b19f887ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f71a72d9eb0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Etapa 2: Construir modelo"
      ],
      "metadata": {
        "id": "cxP8toDUhACJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class feature(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "      super().__init__()\n",
        "      # load resnet-18\n",
        "      resnet18 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "      #primeiras camadas conv do resnet\n",
        "      self.layer1 = nn.Sequential(\n",
        "          resnet18.conv1,\n",
        "          resnet18.bn1,\n",
        "          resnet18.relu,\n",
        "          resnet18.maxpool\n",
        "        )\n",
        "      self.layer2 = nn.Sequential(\n",
        "          resnet18.conv1,\n",
        "          resnet18.bn1,\n",
        "          resnet18.relu,\n",
        "          resnet18.maxpool\n",
        "        )\n",
        "      self.layer1.training = False\n",
        "  def forward(self,X):\n",
        "    X_ = self.layer1(X)\n",
        "    return X_\n",
        "\n",
        "class discriminator(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "      super().__init__()\n",
        "      self.flatten = nn.Flatten()\n",
        "      size = 115200\n",
        "      self.dense1 = nn.Linear(in_features=size,out_features=128)\n",
        "      self.dense2 = nn.Linear(in_features=128,out_features=36)\n",
        "      self.dense3 = nn.Linear(in_features=36,out_features=1)\n",
        "      self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "  def forward(self,X1,X2):\n",
        "    X1_ = self.flatten(X1)\n",
        "    X2_ = self.flatten(X2)\n",
        "    # X1 = X1.view(X1.size()[0], -1)\n",
        "    # X2 = X2.view(X2.size()[0], -1)\n",
        "    x = torch.cat((X1_,X2_), 1)\n",
        "    x = self.dropout(F.leaky_relu(self.dense1(x),0.2))    \n",
        "    x = self.dropout(F.leaky_relu(self.dense2(x),0.2))\n",
        "    x = self.dropout(F.leaky_relu(self.dense3(x),0.2))\n",
        "    return x\n",
        "\n",
        "class siamese(nn.Module):\n",
        "      def __init__(self) -> None:\n",
        "          super().__init__()\n",
        "          self.feature = feature()      \n",
        "          self.discriminator = discriminator()\n",
        "\n",
        "      def forward(self, X1, X2):          \n",
        "          X1_ = self.feature(X1)\n",
        "          X2_ = self.feature(X2)\n",
        "          X = self.discriminator(X1_, X2_)\n",
        "          return X"
      ],
      "metadata": {
        "id": "lZer_dWYevhh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class DeepNeuralInterface(ABC,nn.Module):\n",
        "    @abstractmethod\n",
        "    def show(self):\n",
        "      pass\n",
        "    @abstractmethod\n",
        "    def get_transforms(self):\n",
        "      pass\n",
        "      \n",
        "class TrainSiameseInterface(ABC):\n",
        "    @abstractmethod\n",
        "    def train(self,net:nn.Module,X)->None:\n",
        "      pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def train_epoch(self,net:nn.Module,X)->None:\n",
        "      pass\n",
        "\n",
        "class teste(TrainSiameseInterface):\n",
        "  def __init__(self,device) -> None:\n",
        "      super().__init__()\n",
        "      self.device = device\n",
        "\n",
        "  def train(self,net,train_loader,epochs=15)->None:\n",
        "    optimizer = optim.Adam(net.parameters(),lr=0.002)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    net.to(device)\n",
        "    for epoch in range(epochs):\n",
        "      D_running_loss = 0\n",
        "      G_running_loss = 0\n",
        "\n",
        "      for i,(imagens_reais,_) in enumerate(train_loader):\n",
        "\n",
        "        batch_size= imagens_reais.size(0)\n",
        "                   \n",
        "        imagens_reais = imagens_reais * 2-1       \n",
        "        imagens_reais = imagens_reais.to(self.device)\n",
        "\n",
        "        #treinamento com reais\n",
        "        outputs_reais = net.forward(imagens_reais,imagens_reais)\n",
        "        labels_reais = (torch.ones(batch_size)*0.9).to(device)\n",
        "        D_loss_reais = criterion(outputs_reais.view(*labels_reais.shape),labels_reais)      \n",
        "\n",
        "        #treinamento com falso  \n",
        "        imagens_ruido = torch.flip(imagens_reais, [0,1])\n",
        "        outputs_falsos = net.forward(imagens_reais,imagens_ruido)\n",
        "        labels_falsos= torch.zeros(batch_size).to(device)\n",
        "        D_loss_falsos = criterion(outputs_reais.view(*labels_falsos.shape),labels_falsos)\n",
        "\n",
        "        D_loss = D_loss_reais+ D_loss_falsos\n",
        "        D_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        D_running_loss += D_loss.item()\n",
        "        # print('train ',i,imagens_reais.shape,D_loss_reais,D_loss_falsos)\n",
        "\n",
        "      D_running_loss /=len(train_loader)\n",
        "      print('epoch -> {} loss -> {}'.format(epoch,D_running_loss)) \n",
        "\n",
        "\n",
        "  def train_epoch(self,net,X)->None:\n",
        "    pass\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hirfPaX-Nyhw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#criar transformador\n",
        "#transform = transforms.ToTensor()\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(128),\n",
        "    transforms.CenterCrop(120),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n"
      ],
      "metadata": {
        "id": "P-GtI6qPT64g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "from skimage.draw import rectangle\n",
        "from roboflow import Roboflow\n",
        "from pylabel import importer\n",
        "from PIL import Image\n",
        "class FibersDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Fibers dataset.\"\"\"\n",
        "\n",
        "    def __init__(self,transform=None):\n",
        "      rf = Roboflow(api_key=\"JhqX7HlUL57cmzbBqIav\")\n",
        "      project = rf.workspace().project(\"fibberpaper\")\n",
        "      dataset = project.version(1).download(\"yolov5\")\n",
        "      dataset = importer.ImportYoloV5(path='/content/FibberPaper-1/test/labels', path_to_images='/content/FibberPaper-1/test/images')\n",
        "      self.mapa,self.data = self.extractFibersOfImagens(dataset.df)\n",
        "      self.sourceTransform= transform\n",
        "      self.rootDir = '.'\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "            \n",
        "        image = Image.fromarray(self.data[idx])\n",
        "        if self.sourceTransform:\n",
        "            image = self.sourceTransform(image)\n",
        "\n",
        "        class_id = torch.tensor([0])\n",
        "        return image,class_id\n",
        "\n",
        "\n",
        "    #extrar o conjunto de pixel das fibras contidas no boundbox descrito no dataframe\n",
        "    def extractFibersOfImagens(self,df):\n",
        "      mapa = {}\n",
        "      fibers = []\n",
        "      #para cada linha \n",
        "      for index, row in df.iterrows():\n",
        "        if \"idoc\" in row.img_filename: \n",
        "          continue\n",
        "        #ler a imagem e salva no map\n",
        "        path_full='{}/{}'.format(row.img_folder,row.img_filename)\n",
        "        if mapa.get(row.img_filename) is None:\n",
        "          img = io.imread(path_full)\n",
        "          mapa[row.img_filename]=img\n",
        "        #recorta o box da fibra\n",
        "        x1,y1,x2,y2 = int(row.ann_bbox_xmin),int(row.ann_bbox_ymin),int(row.ann_bbox_xmax),int(row.ann_bbox_ymax)\n",
        "        fibers.append(mapa[row.img_filename][y1:y2,x1:x2])\n",
        "        #normaliza a imagem com o transforms\n",
        "        #tenta remover o fundo\n",
        "      return mapa,np.array(fibers)\n",
        "\n",
        "\n",
        "train = FibersDataset(transform=preprocess)\n",
        "# criar loader\n",
        "train_loader = torch.utils.data.DataLoader(train,batch_size=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "cUdN5piCHrdr",
        "outputId": "d8ebe71e-7464-4be7-b0bc-4dbb4d8ef36d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b761b9c40c8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrectangle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mroboflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRoboflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpylabel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimporter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'roboflow'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criação dos objetos"
      ],
      "metadata": {
        "id": "ughFLqD05w-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = siamese()\n",
        "param = net.parameters()\n",
        "optimizer = optim.Adam(param,lr=0.002)\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "qWcyafR-kGUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')"
      ],
      "metadata": {
        "id": "lczWDOsS8q2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teste(device=device).train(net=net,train_loader=train_loader,epochs=500)"
      ],
      "metadata": {
        "id": "VK7oKfkkhWW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img1 =(3 , 120, 120)\n",
        "summary(net,[img1,img1])"
      ],
      "metadata": {
        "id": "WzFRx6e18uzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Separa primeiras camadas"
      ],
      "metadata": {
        "id": "7RF3dd3N_fNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### visualiza resultado"
      ],
      "metadata": {
        "id": "9GIFiWQn_3MF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_result(input_image,conv_unif):\n",
        "  plt.style.use('classic')\n",
        "  plt.imshow(input_image.transpose(2, 0))\n",
        "  plt.show()\n",
        "  plt.imshow(conv_unif)\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "ZA6Gwk8E8dHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testa sub modelo em um exemplo real"
      ],
      "metadata": {
        "id": "M6Lmi6za_pKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_image,_ in train:\n",
        "  # input_image,_ = next(iter(train))\n",
        "  input_batch = input_image.unsqueeze(0) \n",
        "  print(input_batch.shape)\n",
        "\n",
        "  model1 = feature()\n",
        "  # move the input and model to GPU for speed if available\n",
        "  if torch.cuda.is_available():\n",
        "      input_batch = input_batch.to('cuda')\n",
        "      model1.to('cuda')\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output = model1(input_batch)\n",
        "      out1=output.cpu().numpy()\n",
        "  conv_unif = np.sum(out1[0],axis=0)/out1[0].shape[0]\n",
        "  print(conv_unif.shape)\n",
        "  plot_result(input_image,conv_unif)"
      ],
      "metadata": {
        "id": "RG3xxNE7-mC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# testar rede siamesa\n"
      ],
      "metadata": {
        "id": "kGbk_yymbysK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### concatena features da camada de saida"
      ],
      "metadata": {
        "id": "B2V-NImB_w6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch = next(iter(train_loader))\n",
        "for img,_ in train_loader:\n",
        "  print(t)"
      ],
      "metadata": {
        "id": "pa6CeoywfxlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for input_image,_ in train:\n",
        "  # input_image,_ = next(iter(train))\n",
        "  input_batch = input_image.unsqueeze(0) \n",
        "  print(input_batch.shape)\n",
        "  # move the input and model to GPU for speed if available\n",
        "  if torch.cuda.is_available():\n",
        "      input_batch = input_batch.to('cuda')\n",
        "      net = net.to('cuda')\n",
        "  with torch.no_grad():\n",
        "      output = net(input_batch,input_batch)\n",
        "      out1=output.cpu().numpy()\n",
        "      print(out1)"
      ],
      "metadata": {
        "id": "_Ew-Z4rK9F6s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "siamese",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}